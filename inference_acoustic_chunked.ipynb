{
  "cells": [
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "# Smart Phoneme-Based Chunked Inference\n",
        "This notebook implements smart chunked inference that finds silence regions in mel spectrogram for natural chunk boundaries.\n"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## 1. Import Libraries and Setup\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [],
      "source": [
        "#!/usr/bin/env python3\n",
        "from diffwave.inference import predict as diffwave_predict\n",
        "import torch\n",
        "import numpy as np\n",
        "import IPython.display as ipd\n",
        "import torchaudio\n",
        "import soundfile as sf\n",
        "import librosa\n",
        "from pymcd.mcd import Calculate_MCD\n",
        "from pystoi import stoi\n",
        "from pesq import pesq\n",
        "import time\n",
        "\n",
        "# Set CUDA device\n",
        "import os\n",
        "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"1\"\n"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## 2. Define Silence Detection Functions\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [],
      "source": [
        "def find_silence_boundaries(mel_spectrogram, silence_threshold_db=-40, min_silence_frames=5):\n",
        "    \"\"\"\n",
        "    Find silence regions in mel spectrogram for smart chunking boundaries\n",
        "    \n",
        "    Args:\n",
        "        mel_spectrogram: [batch, n_mels, time_frames]\n",
        "        silence_threshold_db: dB threshold for silence detection\n",
        "        min_silence_frames: Minimum consecutive silent frames to consider as boundary\n",
        "    \n",
        "    Returns:\n",
        "        List of frame indices where silence regions occur (good chunk boundaries)\n",
        "    \"\"\"\n",
        "\n",
        "    energy_per_frame = torch.mean(mel_spectrogram, dim=1).squeeze()  # Average energy across mel bins\n",
        "    energy_db = (energy_per_frame * 100) - 100  #unnormalize, this will differ based on vocoder - typically energy is not normalized anyways, it's a quirk of diffWave\n",
        "    \n",
        "    # Find silent frames\n",
        "    silent_frames = energy_db < silence_threshold_db\n",
        "    \n",
        "    # Find silence regions (consecutive silent frames)\n",
        "    silence_boundaries = []\n",
        "    in_silence = False\n",
        "    silence_start = 0\n",
        "    \n",
        "    for i, is_silent in enumerate(silent_frames):\n",
        "        if is_silent and not in_silence:\n",
        "            # Start of silence region\n",
        "            silence_start = i\n",
        "            in_silence = True\n",
        "        elif not is_silent and in_silence:\n",
        "            # End of silence region\n",
        "            silence_length = i - silence_start\n",
        "            if silence_length >= min_silence_frames:\n",
        "                # Use middle of silence region as boundary\n",
        "                boundary = silence_start + silence_length // 2\n",
        "                silence_boundaries.append(boundary)\n",
        "            in_silence = False\n",
        "    \n",
        "    # FIX: Handle case where spectrogram ends in silence\n",
        "    if in_silence:\n",
        "        silence_length = len(silent_frames) - silence_start\n",
        "        if silence_length >= min_silence_frames:\n",
        "            boundary = silence_start + silence_length // 2\n",
        "            silence_boundaries.append(boundary)\n",
        "    \n",
        "    # Always include start and end\n",
        "    if 0 not in silence_boundaries:\n",
        "        silence_boundaries.insert(0, 0)\n",
        "    if len(energy_per_frame) - 1 not in silence_boundaries:\n",
        "        silence_boundaries.append(len(energy_per_frame))  # Use len() not len()-1 for proper slicing\n",
        "    \n",
        "    return sorted(silence_boundaries)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## 3. Define Smart Chunked Prediction Function\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def smart_chunked_predict(mel_spectrogram, model_dir, max_chunk_frames=512, fast_sampling=True):\n",
        "    \"\"\"\n",
        "    Smart chunked inference using silence boundaries for natural phoneme chunking\n",
        "    \"\"\"\n",
        "    print(\"=== Smart Phoneme-Based Chunked Inference ===\")\n",
        "    print(f\"Input mel shape: {mel_spectrogram.shape}\")\n",
        "    \n",
        "    # Find natural silence boundaries\n",
        "    boundaries = find_silence_boundaries(mel_spectrogram, silence_threshold_db=-40, min_silence_frames=5)\n",
        "    \n",
        "    # FIX: Create chunks between boundaries with proper logic\n",
        "    chunks = []\n",
        "    \n",
        "    for i in range(len(boundaries) - 1):\n",
        "        chunk_start = boundaries[i]\n",
        "        chunk_end = boundaries[i + 1]\n",
        "        \n",
        "        # FIX: Skip zero-length chunks\n",
        "        if chunk_start >= chunk_end:\n",
        "            continue\n",
        "            \n",
        "        chunk_size = chunk_end - chunk_start\n",
        "        \n",
        "        if chunk_size <= max_chunk_frames:\n",
        "            # Chunk fits within limit\n",
        "            chunks.append((chunk_start, chunk_end))\n",
        "        else:\n",
        "            # Chunk too big, split it at max size\n",
        "            current_start = chunk_start\n",
        "            while current_start + max_chunk_frames < chunk_end:\n",
        "                chunks.append((current_start, current_start + max_chunk_frames))\n",
        "                current_start += max_chunk_frames\n",
        "            # Add remaining part (if any)\n",
        "            if current_start < chunk_end:\n",
        "                chunks.append((current_start, chunk_end))\n",
        "    \n",
        "    print(f\"Found {len(boundaries)} silence boundaries\")\n",
        "    print(f\"Created {len(chunks)} smart chunks\")  # FIX: Log chunks not boundaries\n",
        "    \n",
        "    # Process each chunk\n",
        "    audio_segments = []\n",
        "    sample_rate = None\n",
        "    \n",
        "    start_time = time.time()\n",
        "    \n",
        "    for i, (start_frame, end_frame) in enumerate(chunks):\n",
        "        print(f\"Processing chunk {i+1}/{len(chunks)}: frames {start_frame}-{end_frame}\")\n",
        "        \n",
        "        # Extract chunk\n",
        "        mel_chunk = mel_spectrogram[:, :, start_frame:end_frame]\n",
        "        \n",
        "        # Generate audio\n",
        "        audio_chunk, sr = diffwave_predict(mel_chunk, model_dir, fast_sampling=fast_sampling)\n",
        "        \n",
        "        if sample_rate is None:\n",
        "            sample_rate = sr\n",
        "            \n",
        "        audio_segments.append(audio_chunk.squeeze().cpu())\n",
        "        \n",
        "        # Free memory\n",
        "        del mel_chunk, audio_chunk\n",
        "    \n",
        "    # FIX: Single synchronize at end for timing accuracy\n",
        "    torch.cuda.synchronize()\n",
        "    \n",
        "    # Concatenate all chunks (no overlap-add needed since we used silence boundaries)\n",
        "    combined_audio = torch.cat(audio_segments, dim=0)\n",
        "    \n",
        "    total_time = time.time() - start_time\n",
        "    print(f\"Total processing time: {total_time:.3f}s\")\n",
        "    print(f\"Final audio shape: {combined_audio.shape}\")\n",
        "    \n",
        "    return combined_audio.unsqueeze(0), sample_rate\n"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## 4. Load Model and Spectrogram Data\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Configuration\n",
        "model_dir = 'diffwave-ljspeech-22kHz-1000578.pt'\n",
        "spectrogram_data = torch.from_numpy(np.load('mel_spectrogram_data.npy')).float().unsqueeze(0).to('cuda')\n",
        "spectrogram = spectrogram_data\n",
        "\n",
        "print(f\"Loaded spectrogram with shape: {spectrogram.shape}\")\n",
        "print(f\"Model path: {model_dir}\")\n"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## 5. Run Smart Chunked Inference\n"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## 11. Comprehensive Benchmarking Framework\n",
        "\n",
        "This section implements systematic benchmarking across multiple clip lengths with statistical analysis.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Smart chunked inference with timing\n",
        "start_time = time.time()\n",
        "audio, sample_rate = smart_chunked_predict(spectrogram, model_dir, max_chunk_frames=512, fast_sampling=True)\n",
        "torch.cuda.synchronize()\n",
        "end_time = time.time()\n",
        "inference_time = end_time - start_time\n",
        "\n",
        "print(f\"\\nTotal inference time: {inference_time:.3f} seconds\")\n"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## 6. Save Generated Audio and Display\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Prepare audio for playback and save\n",
        "audio_for_playback = audio.squeeze().cpu()\n",
        "torchaudio.save(\"chunked_diffwave.wav\", audio_for_playback.unsqueeze(0), sample_rate)\n",
        "\n",
        "# Display audio player\n",
        "ipd.display(ipd.Audio(audio_for_playback, rate=sample_rate))\n",
        "\n",
        "print(f\"Generated audio saved as: chunked_diffwave.wav\")\n",
        "print(f\"Audio duration: {len(audio_for_playback) / sample_rate:.3f} seconds\")\n"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## 7. Calculate MCD Score\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Setup file paths\n",
        "original_file = \"example.wav\"\n",
        "generated_file = \"chunked_diffwave.wav\"\n",
        "\n",
        "print(\"\\n--- Running Vocoder Benchmarks ---\")\n",
        "print(f\"Original: {original_file}\")\n",
        "print(f\"Generated: {generated_file}\\n\")\n",
        "\n",
        "# MCD (Mel Cepstral Distortion) - Lower is better\n",
        "mcd_toolbox = Calculate_MCD(MCD_mode=\"dtw\")\n",
        "mcd_value = mcd_toolbox.calculate_mcd(original_file, generated_file)\n",
        "print(f\"MCD↓:  {mcd_value:.2f} dB\")\n"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## 8. Calculate STOI Score\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# STOI (Short-Time Objective Intelligibility) - Higher is better\n",
        "original_audio, sr = sf.read(original_file)\n",
        "generated_audio, sr_gen = sf.read(generated_file)\n",
        "if sr != sr_gen:\n",
        "    generated_audio = librosa.resample(y=generated_audio, orig_sr=sr_gen, target_sr=sr)\n",
        "\n",
        "min_len = min(len(original_audio), len(generated_audio))\n",
        "original_audio = original_audio[:min_len]\n",
        "generated_audio = generated_audio[:min_len]\n",
        "\n",
        "stoi_score = stoi(original_audio, generated_audio, sr, extended=False)\n",
        "print(f\"STOI↑:  {stoi_score:.4f}\")\n"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## 9. Calculate PESQ Score\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# PESQ (Perceptual Evaluation of Speech Quality) - Higher is better\n",
        "sr_pesq = 16000\n",
        "original_16k = librosa.resample(y=original_audio, orig_sr=sr, target_sr=sr_pesq)\n",
        "generated_16k = librosa.resample(y=generated_audio, orig_sr=sr, target_sr=sr_pesq)\n",
        "pesq_score = pesq(sr_pesq, original_16k, generated_16k, 'wb')\n",
        "print(f\"PESQ↑:  {pesq_score:.2f}\")\n"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## 10. Performance Metrics and RTF Calculation\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Performance metrics\n",
        "print(\"\\n--- Performance Metrics ---\")\n",
        "audio_duration = len(audio_for_playback) / sample_rate\n",
        "rtf = inference_time / audio_duration\n",
        "print(f\"Audio duration: {audio_duration:.3f} seconds\")\n",
        "print(f\"Inference time: {inference_time:.3f} seconds\")\n",
        "print(f\"RTF: {rtf:.3f}\")\n",
        "print(f\"Method: Smart Phoneme-Based Chunking with Silence Detection\")\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "diffenv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
